% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_constructors.R
\name{multi_NODE}
\alias{multi_NODE}
\title{Define a NODE model with multiple time series}
\usage{
multi_NODE(
  data,
  covariates = NULL,
  time_column_name = "time",
  series_column_name = "series",
  hidden_units = 10,
  seed = 1,
  proc_weight = 1,
  obs_weight = 1,
  reg_weight = 10^-6,
  reg_type = "L2",
  l = 0.25,
  extrap_rho = 0.1,
  bayesian = FALSE,
  uid = gsub(x = format(Sys.time(), "\%Y\%m\%d\%H\%M\%OS6"), pattern = "[.]", replacement
    = "")
)
}
\arguments{
\item{data}{A data frame of observed state variables over time.}

\item{covariates}{A data frame of observed covariates (e.g., environmental
conditions) over time. This data frame must have the same column
name for time as the primary dataset, but the time points do not need to
match because the values of the covariates between time points included in
the data frame \code{covariates} are interpolated using a linear spline. Optional.}

\item{time_column_name}{The column in \code{data} and \code{covariates} that contains
the time data, indicating when the observations were made.}

\item{series_column_name}{The column in \code{data} and \code{covariates} that contains
the series data, indicating the identifying information for the observations.}

\item{hidden_units}{Number of neurons in the single hidden layer.}

\item{seed}{Fixed random seed for repeatable results.}

\item{proc_weight}{Weight of the process error term \eqn{\nu_t} in the loss
function. The process weight controls how closely the model predictions
match the state estimates \eqn{\hat{u}_t}.}

\item{obs_weight}{Weight of the observation error term \eqn{\epsilon_t} in the loss
function. The observation weight controls how closely the state estimates
\eqn{\hat{u}_t} match the observations \eqn{y_t}. Smaller values of the observation weight
correspond to datasets with larger amounts of observation error and vice versa.}

\item{reg_weight}{Weight \eqn{\lambda} of the regularization penalty term in the loss
function.}

\item{reg_type}{Type of regularization used to mitigate overfitting.
Options are either "L1" (LASSO) or "L2" (ridge regression). The penalty term
added to the loss function is either the absolute value of the sum of
coefficients (L1) or the squared sum of coefficients (L2). Generally, the
default of "L2" should be used.}

\item{l}{Extrapolation length scale parameter for forecasting. \code{l} controls
how quickly correlations decay with distance between points (i.e., how wiggly the function is).
Small values lead to fast decay and the extrapolation reverts to the prior mean
quickly beyond the observed data. Large values lead to slow decay and the extrapolation
stays similar to the last trend for a longer period.}

\item{extrap_rho}{Extrapolation marginal SD parameter for forecasting.
\code{extrap_rho} controls the magnitude of the extrapolation. Small values lead to
narrow confidence intervals, large values lead to wide confidence intervals.}

\item{bayesian}{Logical (\code{TRUE} or \code{FALSE}) for whether or not the UDE is a
Bayesian UDE.}

\item{uid}{A string that serves as a unique identifier to save the
model into Julia. It is not recommended to modify this parameter.}
}
\value{
An untrained NODE model containing all the defined parameters.
}
\description{
\code{multi_NODE()} constructs a neural ordinary differential equation (NODE) model
for multiple time series. NODEs use neural networks to learn unknown
nonlinear relationships from time series data. \code{multi_NODE()} builds
a continuous-time UDE for state variables \eqn{u_t} and covariates \eqn{X_t}
in series \eqn{i} using a neural network, with weights \eqn{w} and
biases \eqn{b}, to represent the right-hand side of the differential equation
\deqn{\frac{du}{dt} = NN(u_{i,t},x_{i,t};w,b)}
}
\examples{
print("test")
}
