---
title: "emude at ESA 2025"
author: "Zechariah Meunier"
date: '`r Sys.Date()`'
output: html_document
---

# Setup

Load R package dependencies

```{r}
library(emude)
library(JuliaCall)
library(ModelMetrics)
library(tidyverse)
library(deSolve)
```

Set up Julia and emude

```{r}
# Change the file path to your Julia executable. Mind the slashes! 
julia_setup(JULIA_HOME = "C:/Users/zdmeu/.julia/juliaup/julia-1.11.4+0.x64.w64.mingw32/bin")
emude_setup()
```

# Case Study: Canadian Lynx-Hare Predator-Prey Cycles

## Data Exploration

We will work with two different versions of the lynx-hare time series. The first spanning 1845--1935 was documented by Duncan A. MacLulich in his book *Fluctuations in the Numbers of the Varying Hare (Lepus americanus)* published in 1937. The second spanning 1847--1903 was documented by Egbert G. Leigh, Jr. in Table III of his chapter entitled "The Ecological Role of Volterra's Equations" in the book *Some Mathematical Problems in Biology* published in 1968.

The MacLulich (1937) dataset records the number of pelts in the thousands, while the Leigh (1968) dataset records the raw number of pelts. Both datasets have large variances, so we will apply the **emude functions** `rel_colmax()` and `rel_minmax()` as two possible methods for rescaling.

```{r}
lynxhare1 <- read.csv("MacLulich 1937.csv")
lynxhare2 <- read.csv("Leigh 1968.csv")
#perform relativization by column maximum
?rel_colmax()
lynxhare1_rel <- rel_colmax(lynxhare1, time_column_name = "Year")
lynxhare2_rel <- rel_colmax(lynxhare2, time_column_name = "Year")
#perform min-max normalization
?rel_minmax()
lynxhare1_norm <- rel_minmax(lynxhare1, time_column_name = "Year")
lynxhare2_norm <- rel_minmax(lynxhare2, time_column_name = "Year")
```

Given the different data sources, it is interesting to see how they differ. Here we perform a simple left join by year, and then plot the data points and simple linear regressions as compared to a 1:1 line.

```{r}
lynxhare <- left_join(lynxhare1, lynxhare2, by = "Year")

ggplot(data = lynxhare) +
  geom_abline(intercept = 0, slope = 1000) +
  geom_smooth(method = "lm", aes(x = Hare.x, y = Hare.y, color = "1")) +
  geom_smooth(method = "lm", aes(x = Lynx.x, y = Lynx.y, color = "2")) +
  geom_point(aes(x = Hare.x, y = Hare.y, color = "1"), alpha = 0.5) +
  geom_point(aes(x = Lynx.x, y = Lynx.y, color = "2"), alpha = 0.5) +
  scale_x_continuous(name = "MacLulich 1937 population (thousand pelts)") +  
  scale_y_continuous(name = "Leigh 1968 population (pelts)") +
  scale_color_manual(name = "",
                     labels = c("Hare","Lynx"),
                     values = c("dodgerblue","firebrick")) +
  theme_bw() +
  theme(legend.position = "top")
```

An excellent correspondence for the hare counts and a fairly good correspondence for the lynx counts.

Next we will plot the two time series using the **emude function** `series_plot()`. This leverages ggplot functionality but it expects long-format data, so we need to pivot the joined data frame longer and do some additional data wrangling before plotting.

```{r}
?series_plot()

lynxhare_long <- pivot_longer(data = lynxhare,
                              cols = c(Hare.x, Hare.y, Lynx.x, Lynx.y),
                              names_to = "Species",
                              values_to = "Population") %>%
  mutate(
    Source = case_match(
      Species,
      "Hare.x" ~ "MacLulich 1937", "Lynx.x" ~ "MacLulich 1937",
      "Hare.y" ~ "Leigh 1968", "Lynx.y" ~ "Leigh 1968"),
    Species = case_match(
      Species,
      "Hare.x" ~ "Hare", "Lynx.x" ~ "Lynx",
      "Hare.y" ~ "Hare", "Lynx.y" ~ "Lynx")
  )

series_plot(observations = lynxhare_long, 
            x = Year, y = Population, group = Species) +
  facet_wrap(~Source, scales = "free", nrow = 2) +
  scale_x_continuous(breaks = seq(1845, 1935, 10)) +
  scale_color_manual(name = "",
                     labels = c("Hare","Lynx"),
                     values = c("dodgerblue","firebrick"))
```

## Training and Testing Split

As in many machine learning applications, we should split our data into training and testing sets. A good rule of thumb is an 80%/20% split between training and testing data. Given that our two datasets are 91 and 57 years in length, our splits are 72/19 years and 46/11 years, respectively.

```{r}
# List of datasets to split
datasets <- list(
  lynxhare1       = lynxhare1,
  lynxhare2       = lynxhare2,
  lynxhare1_rel   = lynxhare1_rel,
  lynxhare2_rel   = lynxhare2_rel,
  lynxhare1_norm  = lynxhare1_norm,
  lynxhare2_norm  = lynxhare2_norm
)

# Function to split into train/test (80/20)
split_data <- function(df, train_frac = 0.8) {
  split_idx <- floor(train_frac * nrow(df))
  train <- df[1:split_idx, ]
  test  <- df[(split_idx + 1):nrow(df), ]
  list(train = train, test = test)
}

# Apply splitting to each dataset
splits <- lapply(datasets, split_data)

# Store the train/test sets
lynxhare1_train <- splits$lynxhare1$train
lynxhare1_test  <- splits$lynxhare1$test

lynxhare2_train <- splits$lynxhare2$train
lynxhare2_test  <- splits$lynxhare2$test

lynxhare1_rel_train <- splits$lynxhare1_rel$train
lynxhare1_rel_test  <- splits$lynxhare1_rel$test

lynxhare2_rel_train <- splits$lynxhare2_rel$train
lynxhare2_rel_test  <- splits$lynxhare2_rel$test

lynxhare1_norm_train <- splits$lynxhare1_norm$train
lynxhare1_norm_test  <- splits$lynxhare1_norm$test

lynxhare2_norm_train <- splits$lynxhare2_norm$train
lynxhare2_norm_test  <- splits$lynxhare2_norm$test
```

## Neural Ordinary Differential Equation (NODE)

Now we will use scientific machine learning (SciML) methods to model our time series. The first method is called a neural ordinary differential equation (NODE), which uses an artificial neural network to learn unknown nonlinear relationships. The NODE model employs a neural network with weights $w$ and biases $b$ to represent the right-hand side of a system of ODEs with state variables $u_t$ and optional covariates $X_t$:

$$
\frac{du}{dt} = NN(u_t,X_t;w,b)
$$

### Model Creation, Cross-Validation, and Training

We will use the **emude function** `NODE()` to create six NODE models, one each for:

-   datasets 1 and 2 with raw abundances (note the warnings)
-   datasets 1 and 2 with populations relativized by column maximum
-   datasets 1 and 2 with populations rescaled by min-max normalization

```{r}
?NODE()

# raw data, no rescaling
NODE1 <- NODE(data = lynxhare1_train, time_column_name = "Year")
NODE2 <- NODE(data = lynxhare2_train, time_column_name = "Year")

# relativization by column maximum
NODE1_rel <- NODE(data = lynxhare1_rel_train, time_column_name = "Year")
NODE2_rel <- NODE(data = lynxhare2_rel_train, time_column_name = "Year")

# min-max normalization
NODE1_norm <- NODE(data = lynxhare1_norm_train, time_column_name = "Year")
NODE2_norm <- NODE(data = lynxhare2_norm_train, time_column_name = "Year")
```

Next, we'll perform leave-future-out cross-validation (cv) with the **emude function** `cross_validation()` for all six models. You can write the results to a file path, but we will just save them in our environment and then wrangle the output into a data frame.

```{r}
?cross_validation()

NODE1_cv <- cross_validation(model = NODE1)
NODE2_cv <- cross_validation(model = NODE2)
NODE1_rel_cv <- cross_validation(model = NODE1_rel)
NODE2_rel_cv <- cross_validation(model = NODE2_rel)
NODE1_norm_cv <- cross_validation(model = NODE1_norm)
NODE2_norm_cv <- cross_validation(model = NODE2_norm)

NODE_combined_results <- list(
  NODE1 = NODE1_cv[[1]],
  NODE2 = NODE2_cv[[1]],
  NODE1_rel = NODE1_rel_cv[[1]],
  NODE2_rel = NODE2_rel_cv[[1]],
  NODE1_norm = NODE1_norm_cv[[1]],
  NODE2_norm = NODE2_norm_cv[[1]]
)

NODE_cv_results <- bind_rows(NODE_combined_results, .id = "model")
```

Now we can compare our models for the original and rescaled datasets using the cross-validation results. Note that the mean absolute error of the original data is much higher because it was not rescaled, but that doesn't make it worse.

```{r}
# All results as separate panels
ggplot(data = NODE_cv_results,
       aes(x = horizon, y = mean_absolute_error)) +
  geom_ribbon(aes(ymin = mean_absolute_error-standard_error_of_MAE,
                  ymax = mean_absolute_error+standard_error_of_MAE),
              alpha = 0.2) +
  geom_point() +
  geom_line() +
  scale_x_continuous(name = "Forecast horizon (yr)", breaks = seq(1,10,1)) +
  facet_wrap(~model, scales = "free_y")

# Only the rescaled results on one panel
ggplot(data = filter(NODE_cv_results, model %in% c("NODE1_rel","NODE1_norm",
                                                   "NODE2_rel","NODE2_norm")), 
       aes(x = horizon, y = mean_absolute_error, color = model)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(name = "Forecast horizon (yr)", breaks = seq(1,10,1)) +
  theme_bw()
```

Based on the second plot, the normalization produced less error than the relativization for dataset 1, but the relativization was better for dataset 2. We'll focus on the `NODE1_norm` and `NODE2_rel` models for training and one-step ahead predictions. We'll perform this step using the **emude functions** `train_UDE()` and `predict_UDE()`.

```{r}
?train_UDE()
?predict_UDE()

train_UDE(NODE1_norm)
NODE1_predictions <- predict_UDE(NODE1_norm, test_data = lynxhare1_norm_test)

train_UDE(NODE2_rel)
NODE2_predictions <- predict_UDE(NODE2_rel, test_data = lynxhare2_rel_test)
```

### One-step-ahead Predictions

To standardize our assessment of model performance across the two time series, we will calculate the normalized root mean squared error (NRMSE).

$$
NRMSE = \frac{1}{\bar{y}} \sqrt{ \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)^2}
$$

Then, we'll pivot the observations and predictions to long-format data frames. Finally, we will plot the observed data as points and the predictions as lines using the **emude functions** `series_plot()` and `phase_plane_2D()`.

```{r}
?phase_plane_2D()

observations <- list(lynxhare1_norm, lynxhare2_rel)
testsets <- list(lynxhare1_norm_test, lynxhare2_rel_test)
predictions <- list(NODE1_predictions, NODE2_predictions)
subtitles <- c("Dataset 1, min-max normalization, NODE model",
               "Dataset 2, relativization by column max, NODE model")

for (i in 1:2){
  obs <- observations[[i]]
  tst <- testsets[[i]]
  preds <- predictions[[i]]
  
  # calculate the NRMSE separately for each species, removing the first row because the one-step-head predictions do not include the first year
  hareNRMSE = round(rmse(actual = tst[-1,"Hare"],
                         predicted = preds[,"Hare"])/mean(tst[-1,"Hare"]), 2)
  lynxNRMSE = round(rmse(actual = tst[-1,"Lynx"],
                         predicted = preds[,"Lynx"])/mean(tst[-1,"Lynx"]), 2)
  
  # pivot columns longer for plotting
  obs_long <- pivot_longer(data = obs, cols = c(Hare, Lynx),
                           names_to = "Species", values_to = "Population")
  
  preds_long <- pivot_longer(data = preds, cols = c(Hare, Lynx),
                             names_to = "Species", values_to = "Population")
  
  # create plots and add NRMSE values for model fit
  plot1 <- series_plot(observations = obs_long, predictions = preds_long,
                       x = Year, y = Population, group = Species) + 
    annotate(geom = "text", hjust = 1,
             x = max(obs_long$Year, preds_long$Year),
             y = max(obs_long$Population, preds_long$Population) * 0.95,
             label = paste0("Hare NRMSE = ", hareNRMSE, "\n",
                            "Lynx NRMSE = ", lynxNRMSE)) +
    scale_color_manual(name = "",
                       labels = c("Hare","Lynx"),
                       values = c("dodgerblue","firebrick")) +
    labs(subtitle = subtitles[i])
  # create phase plane
  plot2 <- phase_plane_2D(observations = obs_long, predictions = preds_long,
                          names = Species, values = Population,
                          x = Hare, y = Lynx) +
    labs(subtitle = subtitles[i])
  
  print(plot1)
  print(plot2)
}
```

More training data make better models!

## Custom Derivatives

The second SciML method is called a universal differential equation (UDE), which embeds an artificial neural network inside a system of ODEs to learn unknown nonlinear relationships. The UDE model employs a neural network with parameters $\theta$ and a known function $f$ to represent the right-hand side of a system of ODEs with state variables $u_t$ and optional covariates $X_t$:

$$
\frac{du}{dt}=f(u_t,X_t,t,NN(u_t,X_t);\theta)
$$

### Model Creation, Cross-Validation, and Training

We will use the **emude function** `custom_derivatives()` and our user-defined `derivs` function to create six UDE models, as before:

-   datasets 1 and 2 with raw abundances (note the warnings)
-   datasets 1 and 2 with populations relativized by column maximum
-   datasets 1 and 2 with populations rescaled by min-max normalization

In this case, `derivs` is modeled after the Lotka-Volterra predator-prey model:

-   $H$ number of prey (hares)
-   $L$ number of predators (lynx)
-   $\alpha$ prey's per capita growth rate
-   $\beta$ effect of the presence of predators on the prey's death rate
-   $\gamma$ predator's per capita death rate
-   $\delta$ effect of the presence of prey on the predator's growth rate

$$
\frac{dH}{dt} = \alpha H - \beta HL
$$

$$
\frac{dL}{dt} = \delta HL - \gamma L
$$

```{r}
derivs <- function(u,nn,p,t){
  du1 = p$r*u[1] - nn[1]
  du2 = p$theta*nn[1] - p$m*u[2]
  return(c(du1,du2))
}

?custom_derivatives()

# raw data, no rescaling
UDE1 <- custom_derivatives(data = lynxhare1_train,
                           derivs = derivs,
                           initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                           time_column_name = "Year")
UDE2 <- custom_derivatives(data = lynxhare2_train,
                           derivs = derivs,
                           initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                           time_column_name = "Year")

# relativization by column maximum
UDE1_rel <- custom_derivatives(data = lynxhare1_rel_train,
                               derivs = derivs,
                               initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                               time_column_name = "Year")
UDE2_rel <- custom_derivatives(data = lynxhare2_rel_train,
                               derivs = derivs,
                               initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                               time_column_name = "Year")

# min-max normalization
UDE1_norm <- custom_derivatives(data = lynxhare1_norm_train,
                                derivs = derivs,
                                initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                                time_column_name = "Year")
UDE2_norm <- custom_derivatives(data = lynxhare2_norm_train,
                                derivs = derivs,
                                initial_parameters = list(r=0.5, theta=0.1, m = 0.2),
                                time_column_name = "Year")
```

As for the NODE models, we'll perform leave-future-out cross-validation (cv) with the **emude function** `cross_validation()` for all six models. This time, try writing the results to a file path using the `path` argument.

```{r}
UDE1_cv <- cross_validation(model = UDE1)
UDE2_cv <- cross_validation(model = UDE2)
UDE1_rel_cv <- cross_validation(model = UDE1_rel)
UDE2_rel_cv <- cross_validation(model = UDE2_rel)
UDE1_norm_cv <- cross_validation(model = UDE1_norm)
UDE2_norm_cv <- cross_validation(model = UDE2_norm)

UDE_combined_results <- list(
  UDE1 = UDE1_cv[[1]],
  UDE2 = UDE2_cv[[1]],
  UDE1_rel = UDE1_rel_cv[[1]],
  UDE2_rel = UDE2_rel_cv[[1]],
  UDE1_norm = UDE1_norm_cv[[1]],
  UDE2_norm = UDE2_norm_cv[[1]]
)

UDE_cv_results <- bind_rows(UDE_combined_results, .id = "model")
```

Let's again compare our models for the original and rescaled datasets using the cross-validation results. Note that the mean absolute error of the original data is much higher because it was not rescaled, but that doesn't make it worse.

```{r}
# All results as separate panels
ggplot(data = UDE_cv_results,
       aes(x = horizon, y = mean_absolute_error)) +
  geom_ribbon(aes(ymin = mean_absolute_error-standard_error_of_MAE,
                  ymax = mean_absolute_error+standard_error_of_MAE),
              alpha = 0.2) +
  geom_point() +
  geom_line() +
  scale_x_continuous(name = "Forecast horizon (yr)", breaks = seq(1,10,1)) +
  facet_wrap(~model, scales = "free_y")

# Only the rescaled results on one panel
ggplot(data = filter(UDE_cv_results, model %in% c("UDE1_rel","UDE1_norm",
                                                  "UDE2_rel","UDE2_norm")), 
       aes(x = horizon, y = mean_absolute_error, color = model)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(name = "Forecast horizon (yr)", breaks = seq(1,10,1)) +
  theme_bw()
```

Based on the second plot, the best models are `UDE1_rel` and `UDE2_norm` models. We'll use these for training and one-step ahead predictions. As before, we'll perform this step using the **emude functions** `train_UDE()` and `predict_UDE()`.

```{r}
train_UDE(UDE1_rel)
UDE1_predictions <- predict_UDE(UDE1_rel, test_data = lynxhare1_rel_test)

train_UDE(UDE2_norm)
UDE2_predictions <- predict_UDE(UDE2_norm, test_data = lynxhare2_norm_test)
```

### One-step-ahead Predictions

```{r}
observations <- list(lynxhare1_rel, lynxhare2_norm)
testsets <- list(lynxhare1_rel_test, lynxhare2_norm_test)
predictions <- list(UDE1_predictions, UDE2_predictions)
subtitles <- c("Dataset 1, relativization by column max, UDE model",
               "Dataset 2, min-max normalization, UDE model")

for (i in 1:2){
  obs <- observations[[i]]
  tst <- testsets[[i]]
  preds <- predictions[[i]]
  
  # calculate the NRMSE separately for each species, removing the first row because the one-step-head predictions do not include the first year
  hareNRMSE = round(rmse(actual = tst[-1,"Hare"],
                         predicted = preds[,"Hare"])/mean(tst[-1,"Hare"]), 2)
  lynxNRMSE = round(rmse(actual = tst[-1,"Lynx"],
                         predicted = preds[,"Lynx"])/mean(tst[-1,"Lynx"]), 2)
  
  # pivot columns longer for plotting
  obs_long <- pivot_longer(data = obs, cols = c(Hare, Lynx),
                           names_to = "Species", values_to = "Population")
  
  preds_long <- pivot_longer(data = preds, cols = c(Hare, Lynx),
                             names_to = "Species", values_to = "Population")
  
  # create series plots and add NRMSE values for model fit
  plot1 <- series_plot(observations = obs_long, predictions = preds_long,
                       x = Year, y = Population, group = Species) + 
    annotate(geom = "text", hjust = 1,
             x = max(obs_long$Year, preds_long$Year),
             y = max(obs_long$Population, preds_long$Population) * 0.95,
             label = paste0("Hare NRMSE = ", hareNRMSE, "\n",
                            "Lynx NRMSE = ", lynxNRMSE)) +
    scale_color_manual(name = "",
                       labels = c("Hare","Lynx"),
                       values = c("dodgerblue","firebrick")) +
    labs(subtitle = subtitles[i])
  # create phase plane
  plot2 <- phase_plane_2D(observations = obs_long, predictions = preds_long,
                          names = Species, values = Population,
                          x = Hare, y = Lynx) +
    labs(subtitle = subtitles[i])
  
  print(plot1)
  print(plot2)
}
```

Here, more training data and known information about predator-prey dynamics make better models!

As a trade-off for having this structural information about predator-prey dynamics, let's switch our training and testing split from 80/20 to 50/50 and see how the model performs. Jump back to line 116, and then re-run the custom derivatives code chunks!
